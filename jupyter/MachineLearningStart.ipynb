{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 机器学习开始"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 机器学习流程\n",
    "* 获取数据\n",
    "* 数据处理\n",
    "* 特征工程\n",
    "* 机器学习模型算法训练\n",
    "* 模型评估\n",
    "* 应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. scikit-learn数据集API介绍\n",
    "加载数据集：\n",
    "    datasets.load_*()\n",
    "        -获取小规模数据集，数据包含在datasets里面\n",
    "    datasets.fetch_*(data_home = None,subset = 'train')\n",
    "        -获取大规模数据集，需要从网络上下载，data_home参数表示数据集下载的目录，默认~/scikit_learn_data/\n",
    "        -第二个参数表示需要加载的数据集，有训练集和测试集,也可以用'all'表示都要\n",
    "数据集的返回值\n",
    "    datasets.base.Bunch()返回值继承自字典类型，有五个键:\n",
    "        data:特征数据数组，是二维的numpy.ndarray数组\n",
    "        target:标签数组\n",
    "        DESCR:数据描述\n",
    "        featrue_names:特征名\n",
    "        target_names:标签名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鸢尾花数据集:\n",
      " Iris Plants Database\n",
      "====================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML iris datasets.\n",
      "http://archive.ics.uci.edu/ml/datasets/Iris\n",
      "\n",
      "The famous Iris database, first used by Sir R.A Fisher\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      "References\n",
      "----------\n",
      "   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "def datasets_demo():\n",
    "    #sklean数据集的使用\n",
    "    iris = load_iris()\n",
    "    print(\"鸢尾花数据集:\\n\",iris[\"DESCR\"])\n",
    "    return None\n",
    "datasets_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 数据集划分api:\n",
    "    sklearn.model_selection.train_test_split(arrays,*options)\n",
    "        x数据集的特征值\n",
    "        y数据集的标签值\n",
    "        test_size:测试集的大小，用float浮点数。如需测试百分之二十的数据，则传入0.2\n",
    "        randmo_state随机数种子：不同的种子造成不同的采样结果\n",
    "        return 训练集特征值，测试集特征值，训练集目标值，测试集目标值(注意返回结果的顺序)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集的特征值：\n",
      " [[4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [6.7 3.  5.  1.7]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.1 3.5 1.4 0.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  2.  3.5 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [7.7 3.8 6.7 2.2]] (120, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def ds_demo2():\n",
    "    iris = load_iris()\n",
    "    x_train,x_test,y_train,y_test = train_test_split(iris.data, iris.target, test_size = 0.2,random_state = 22)\n",
    "    print(\"训练集的特征值：\\n\" ,x_train,x_train.shape)#shape是指返回的二维数组的行和列\n",
    "ds_demo2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 特征工程介绍\n",
    "> 数据和特征决定了机器学习的上限,而模型和算法只是逼近这个上限而已\n",
    "\n",
    "###### 包含内容\n",
    "1. 特征抽取\n",
    "2. 特征预处理\n",
    "3. 特征降维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 特征提取：将文本、图像等转换成及其学习能够处理得数字特征\n",
    "特征提取api : sklearn.feature_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. 字典特征提取 - 类别 -> one-hot编码\n",
    "sklearn.feature_extraction.DicVectorize(sparse = True...)\n",
    "    - DicVectorize.fit_transform(X) X：字典或包含了字典的迭代器;返回一个sparse矩阵(稀疏矩阵)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_new:\n",
      " [[  1.   0.   0. 100.]\n",
      " [  0.   1.   0.  60.]\n",
      " [  0.   0.   1.  40.]]\n",
      "特征名字:\n",
      " ['city=guangzhou', 'city=shanghai', 'city=shenzhen', 'temp']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "def ds_demo3():\n",
    "    data = [{'city': 'guangzhou','temp':100},{'city':'shanghai','temp': 60},{'city':'shenzhen','temp':40}]\n",
    "    #1.实例化一个转换器类\n",
    "    transfer = DictVectorizer(sparse = False)#默认返回一个稀疏矩阵，为了节省内存\n",
    "    #2.调用transform方法\n",
    "    data_new = transfer.fit_transform(data)\n",
    "    print(\"data_new:\\n\",data_new)\n",
    "    print(\"特征名字:\\n\", transfer.get_feature_names())#符合特征的那一行在data_new中所在列是1，否则为0\n",
    "    return None\n",
    "ds_demo3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.文本特征提取\n",
    "sklearn.feature_extraction.text.CounterVectorizer(stop_words = [])\n",
    "    - stop_words为停用词表，将不需要统计的词放入列表中传进去\n",
    "    - 返回词频矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征值为：\n",
      " [[0 2 0 1 0 1]\n",
      " [1 0 1 0 2 0]]\n",
      "特征名字:\n",
      " ['dislike', 'life', 'like', 'long', 'python', 'short']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def count():\n",
    "    words = [\"life is too long,life is too short\",\"i like python,i dislike python\"]\n",
    "    transfer = CountVectorizer(stop_words = ['is','too'])\n",
    "    data = transfer.fit_transform(words)\n",
    "    print(\"特征值为：\\n\" ,data.toarray())#将稀疏矩阵转为普通矩阵\n",
    "    print(\"特征名字:\\n\",transfer.get_feature_names())\n",
    "count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中文文本的特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征值为：\n",
      " [[0 1 0 0 0 0 0 1 0 1]\n",
      " [0 0 1 0 1 1 0 0 1 0]\n",
      " [1 0 0 1 0 0 1 0 0 0]]\n",
      "特征名字:\n",
      " ['信念', '力量', '可敬', '坚持', '对手', '战胜', '所有', '来自', '耶一耶', '重生']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "#写一个进行分词的函数\n",
    "def cut_word(text):\n",
    "    a = \" \".join(list(jieba.cut(text)))#进行一次强转\n",
    "    #print(a)\n",
    "    return a\n",
    "def chinese():\n",
    "    data = [\"重生的力量来自真我\",\"战胜可敬的对手耶一耶\",\"坚持信念是我的所有\"]\n",
    "    data_new = []\n",
    "    for string in data:\n",
    "        data_new.append(cut_word(string))\n",
    "    transfer = CountVectorizer()\n",
    "    output = transfer.fit_transform(data_new)\n",
    "    print(\"特征值为：\\n\" ,output.toarray())#将稀疏矩阵转为普通矩阵\n",
    "    print(\"特征名字:\\n\",transfer.get_feature_names())\n",
    "chinese()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 关键词：一些词在一篇文章中经常出现，在另一篇文章很少出现，则称为关键词。由此实现对于文章的分类。\n",
    "TF-IDF作用：衡量一个词在一篇文章中的重要程度\n",
    "        TF:即一个词在文章中的词频\n",
    "        IDF：逆向文档频率，是一个词语的普遍重要性的衡量。词用得越普遍，idf值越小。\n",
    "   TF-IDF值 = tf * idf\n",
    "   \n",
    "sklearn.feature_extraction.text.TfidfVectorizer(X)\n",
    "    - X：文本或包含文本的可迭代对象\n",
    "    - 返回词的权重矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4472136  0.2236068  0.2236068  0.2236068  0.2236068  0.2236068\n",
      "  0.2236068  0.2236068  0.2236068  0.2236068  0.2236068  0.\n",
      "  0.         0.4472136  0.2236068  0.2236068  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.6882472\n",
      "  0.22941573 0.         0.         0.         0.6882472 ]]\n",
      "['一直', '为此', '争取', '低迷', '努力', '发展', '呈现', '态势', '提高', '政府', '水平', '生气', '真的', '经济', '经济总量', '这个', '非常']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf():\n",
    "    transfer = TfidfVectorizer()\n",
    "    data = [\"这个月的经济总量一直呈现低迷态势，为此政府一直在努力发展经济，争取提高经济水平\",\"我真的非常生气，非常生气，非常生气\"]\n",
    "    cdata = []\n",
    "    for string in data:\n",
    "        cdata.append(cut_word(string))\n",
    "    data_new = transfer.fit_transform(cdata)\n",
    "    print(data_new.toarray())\n",
    "    print(transfer.get_feature_names())\n",
    "tfidf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征预处理\n",
    "通过一些转换函数将特征数据转换为更加适合算法模型的特征数据，将不同规格的数据转换到同一规格\n",
    "1. 包含内容:\n",
    "    归一化\n",
    "    标准化\n",
    "2. 预处理api\n",
    "    sklearn.preprocessing.MinMaxScaler(feature_range = (0,1))\n",
    "    MinMaxScaler(X)\n",
    "        X:numpy.array 格式:(样本的大小(行数),样本的特征(列数))\n",
    "        return 转换后的numpy.array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "归一化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.         2.         2.         3.        ]\n",
      " [2.8309057  2.34375    2.03225806 2.        ]\n",
      " [3.         3.         3.         2.90909091]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def MinMax():\n",
    "    data = pd.read_csv(\"dating.txt\")\n",
    "    tran = MinMaxScaler(feature_range = (2,3))\n",
    "    newdata = tran.fit_transform(data)\n",
    "    print(newdata)\n",
    "MinMax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 由于归一化受最大值和最小值影响较大，数据容易失真，因此还有标准化处理:\n",
    "x = (x0 - 平均值) / 标准差\n",
    "sklearn.preprocessing.StandaredScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征降维\n",
    "降低随机变量的维度，得到一组不相关的主变量的过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 特征选择  \n",
    "    Filter过滤式  \n",
    "        方差过滤法：低方差特征过滤  \n",
    "        相关系数  \n",
    "    Embeded嵌入式 \n",
    "        决策树  \n",
    "        正则化  \n",
    "        深度学习  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 低方差过滤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn.feature.selection.VarianceThreshold(threshold = 0.0)  \n",
    "    - 将方差低于threshold的特征删除"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 相关系数的计算\n",
    "$$ r = \\frac{n\\sum_{}^{}{xy} - \\sum_{}^{}{x}\\sum_{}^{}{y}}{\\sqrt{n\\sum_{}^{}{x^2}-(\\sum_{}^{}{x})^2}\\sqrt{n\\sum_{}^{}{y^2}-(\\sum_{}^{}{y})^2}} $$  \n",
    "r > 0为正相关，r < 0为负相关  \n",
    "|r|越大，相关性越大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当特征与特征之间相关性很高时：  \n",
    "    1. 选取其中一个  \n",
    "    2. 加权求和  \n",
    "    3. 主成分分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 主成分分析 \n",
    "在降维的过程中，可能会舍弃原有数据，创造新的变量  \n",
    "##### API\n",
    "* sklearn.decomposition.PCA(n_components = None)  \n",
    "    n_components:  \n",
    "        -小数 ：表示保留百分之多少的信息  \n",
    "        -整数 ：减少到多少特征  \n",
    "   PCA.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.22879107e-15  3.82970843e+00]\n",
      " [ 5.74456265e+00 -1.91485422e+00]\n",
      " [-5.74456265e+00 -1.91485422e+00]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def pca():\n",
    "    data = [[2,8,4,5],[6,3,0,8],[5,4,9,1]]\n",
    "    transfer = PCA(n_components = 2)#将4个特征降为两个特征\n",
    "    data_new = transfer.fit_transform(data);\n",
    "    print(data_new)\n",
    "pca()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 案例 ：探究用户对物品类别的喜好细分降维\n",
    "* 找到用户user_id和物品aisle之间的联系\n",
    "    1. 将user_id和aisle放在一张表中 - 合并\n",
    "    2. 找到两者之间的关系 - 交叉表和透视表\n",
    "    3. 特征冗余过多，要进行降维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因数据不足，暂时跳过"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
